{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    ")\n",
    "llm = Ollama(model = 'llama3') \n",
    "chat_model = ChatOllama(model = 'llama3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=ollama_emb)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    " \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, common ways of doing Task Decomposition include:\n",
      "\n",
      "1. Using LLM with simple prompting like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "2. Providing task-specific instructions, such as \"Write a story outline.\" for writing a novel\n",
      "3. Utilizing human inputs\n",
      "\n",
      "These techniques help break down complex tasks into smaller, manageable steps, allowing for more effective planning and execution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique that breaks down complex tasks into smaller, simpler steps. This allows for more manageable problem-solving and sheds light on the model's thinking process. It can be achieved through techniques like Chain of Thought (CoT) or Tree of Thoughts.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is done by LLM with simple prompting, using task-specific instructions, or with human inputs. Common methods include Chain of Thought (CoT), Tree of Thoughts, and few-shot examples to guide LLM in task parsing and planning. These techniques help decompose hard tasks into smaller and simpler steps, making problem-solving more manageable.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is a technique that breaks down complex tasks into smaller, simpler steps. This allows for more manageable problem-solving and sheds light on the model's thinking process. It can be achieved through techniques like Chain of Thought (CoT) or Tree of Thoughts.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition is done by LLM with simple prompting, using task-specific instructions, or with human inputs. Common methods include Chain of Thought (CoT), Tree of Thoughts, and few-shot examples to guide LLM in task parsing and planning. These techniques help decompose hard tasks into smaller and simpler steps, making problem-solving more manageable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=ollama_emb)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chat_model, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(chat_model, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition refers to a technique used in complex tasks, where a hard task is broken down into smaller and more manageable sub-tasks. This approach helps models like LLM-powered autonomous agents plan ahead and think step-by-step, utilizing more test-time computation to achieve better performance on difficult tasks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, common ways of Task Decomposition include:\\n\\n1. Using Large Language Models (LLMs) with simple prompting, such as \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\"\\n2. Utilizing task-specific instructions, like \"Write a story outline.\" for writing a novel\\n3. Incorporating human inputs'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"task decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(chat_model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used in project management, problem-solving, and goal-setting. It involves breaking down a complex task or project into smaller, more manageable sub-tasks or components, often referred to as \"sub-tasks\" or \"child tasks\".\\n\\nThe idea behind task decomposition is that a large, overwhelming task can be made more feasible by dividing it into smaller, less daunting pieces. This approach helps to:\\n\\n1. Simplify the task: By breaking down the task into smaller parts, it becomes easier to understand and tackle.\\n2. Focus on specific goals: Sub-tasks can have clear objectives and deadlines, allowing teams or individuals to concentrate on a specific outcome.\\n3. Improve progress tracking: Task decomposition makes it easier to monitor progress, as each sub-task can be measured independently.\\n4. Enhance collaboration: By having multiple sub-tasks, team members can work simultaneously on different aspects of the project, promoting collaboration and efficiency.\\n\\nTask decomposition is often used in:\\n\\n1. Project management: Breaking down large projects into smaller tasks helps teams stay organized and on track.\\n2. Problem-solving: Decomposing complex problems into simpler sub-issues can make them more manageable and solvable.\\n3. Goal-setting: Decomposing long-term goals into shorter-term objectives can create a roadmap for achieving success.\\n\\nSome examples of task decomposition include:\\n\\n* Breaking down a software development project into smaller components, such as designing the user interface, creating algorithms, or building database infrastructure.\\n* Decomposing a research paper into smaller sections, such as reviewing literature, conducting experiments, and analyzing data.\\n* Dividing a marketing campaign into smaller tasks, such as planning the strategy, creating content, promoting the campaign, and tracking analytics.\\n\\nBy decomposing complex tasks into smaller, manageable pieces, you can make progress more tangible, reduce overwhelm, and ultimately achieve your goals more efficiently.', response_metadata={'token_usage': {'completion_tokens': 373, 'prompt_tokens': 16, 'total_tokens': 389}, 'model_name': 'llama3', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-dfd791cc-4b64-4b4d-939f-d5aa51b93353-0', usage_metadata={'input_tokens': 16, 'output_tokens': 373, 'total_tokens': 389})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = create_react_agent(chat_model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"Hi Bob! It's nice to meet you. How's your day going so far?\", response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 10, 'total_tokens': 29}, 'model_name': 'llama3', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-9475a932-1a58-4bfd-b686-3a9c3cecc2d6-0', usage_metadata={'input_tokens': 10, 'output_tokens': 19, 'total_tokens': 29})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a planning and problem-solving technique that breaks down complex tasks or projects into smaller, manageable, and more tangible pieces (tasks). This process helps to clarify the overall task, identify key steps, and create a roadmap for completion.\\n\\nIn task decomposition, you:\\n\\n1. Define the high-level goal or objective.\\n2. Identify the most important tasks required to achieve that goal.\\n3. Break down each of those tasks into smaller, more specific subtasks (usually 3-5 per task).\\n4. Continue this process until you have a detailed plan with clear actions and deliverables.\\n\\nTask decomposition has many benefits, including:\\n\\n1. Improved understanding: By breaking down complex tasks, you gain clarity on what needs to be done.\\n2. Reduced overwhelm: You\\'ll feel more in control as you focus on one task at a time.\\n3. Increased efficiency: You can start working on individual tasks sooner, without waiting for the entire plan to materialize.\\n4. Enhanced communication: Task decomposition helps teams coordinate efforts and align expectations.\\n\\nFor example, if your goal is to \"Write an article for a new blog,\" task decomposition might look like this:\\n\\n1. Research topic ideas\\n\\t* Identify 5 potential topics\\n\\t* Brainstorm key points for each topic\\n2. Plan the article structure\\n\\t* Determine the article\\'s length and format (e.g., 500 words, 10 sections)\\n\\t* Outline the main points to cover\\n3. Write the article\\n\\t* Write 2-3 draft paragraphs based on research and outline\\n\\t* Revise and edit for clarity and coherence\\n\\nBy breaking down the task of writing an article into these smaller steps, you\\'ll make progress faster, reduce anxiety, and ultimately produce a higher-quality piece.\\n\\nDoes that help clarify task decomposition, Bob?', response_metadata={'token_usage': {'completion_tokens': 366, 'prompt_tokens': 16, 'total_tokens': 382}, 'model_name': 'llama3', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-bb82151e-a791-4ddb-bb14-d78b86fc7091-0', usage_metadata={'input_tokens': 16, 'output_tokens': 366, 'total_tokens': 382})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I dug up the info on common approaches to task decomposition. According to various sources, here are some popular methods:\\n\\n1. **Divide and Conquer**: Break down a large project or task into smaller, independent tasks that can be worked on simultaneously. This approach is particularly useful when you have multiple team members involved.\\n2. **Work Breakdown Structure (WBS)**: A hierarchical decomposition technique that starts with the overall goal and progressively breaks down larger tasks into smaller, more manageable ones. WBS is commonly used in construction, manufacturing, and project management.\\n3. **Task Segmentation**: Divide a task into logical chunks based on specific criteria, such as:\\n\\t* Time-based segmentation (e.g., breaking a project into daily or weekly tasks).\\n\\t* Function-based segmentation (e.g., dividing tasks by responsibility or skill set).\\n\\t* Priority-based segmentation (e.g., categorizing tasks by urgency or importance).\\n4. **Swiss Cheese Method**: Start with the smallest, most manageable tasks and work your way up to the larger ones. This approach helps you make steady progress while avoiding feeling overwhelmed by the scope of the project.\\n5. **The Pomodoro Technique**: Focus on a single task for a short, fixed interval (usually 25 minutes) followed by a brief break. This method helps you stay focused and avoid burnout.\\n\\nThese methods can be used separately or in combination to suit your needs and work style. It's essential to find an approach that resonates with you, as it will help you maintain momentum and motivation throughout the project.\\n\\nWould you like me to elaborate on any of these approaches, Bob?\", response_metadata={'token_usage': {'completion_tokens': 331, 'prompt_tokens': 26, 'total_tokens': 357}, 'model_name': 'llama3', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4f37e26-5ced-4316-8cf6-9f289bb3b713-0', usage_metadata={'input_tokens': 26, 'output_tokens': 331, 'total_tokens': 357})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "chat_model = ChatOpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=ollama_emb)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "\n",
    "agent_executor = create_react_agent(chat_model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
